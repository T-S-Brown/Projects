# Descriptives
descriptives <- train %>%
group_by(Class) %>%
summarise_all(mean)
# Quick and Dirty Evaluation of the principal components - correlation plot
correlations <- train %>%
summarize_all(funs(cor(., train$Class))) %>%
t()
colnames(correlations) <- "coef"
correlations <- data.frame(correlations)
correlations$feature <- rownames(correlations)
rownames(correlations) <- 1:nrow(correlations)
correlations <- mutate(correlations,
direction = case_when(
coef >= 0 ~ "Positive",
TRUE ~ "Negative"
),
magnitude = abs(coef)) %>%
filter(feature != "Class")
correlations$feature <- factor(correlations$feature) %>%
fct_reorder(correlations$magnitude)
exploratory_plot <- ggplot(data = correlations, aes(x = feature, y = magnitude, fill = direction)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Magnitude of Coefficient",
y = "Feature",
title = "Correlations with outcome",
fill = "Coefficient Direction") +
theme(plot.title = element_text(hjust = 0.5))
exploratory_plot
exploratory_plot + ylim(0, 1)
#---------------------------------------#
# Stage 2: Machine Learning Algorithm
#          construction
#---------------------------------------#
# Set up a baseline model: Logistic Regression
# Set up 5 fold cross validation
tc <- trainControl("cv", 5,
savePredictions = TRUE)#,
#classProbs = TRUE)
fit <- train(Class ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
fit <- train(as.factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
install.packages("e1071")
fit <- train(as.factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
View(results)
View(results)
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(as.factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
train$Class
max(train$Class)
target <- parse_factor(train$Class)
target <- parse_factor(c("Genuine", "Fraud"), train$Class)
target <- factor(train$Class)
target <- factor(train$Class)
levels(target) <- c("Fraud", "Genuine")
View(get_correlations)
levels(target) <- c("Fraud", "Genuine")
train$Class <- factor(train$Class)
levels(train$Class) <- c("Fraud", "Genuine")
# Set up 5 fold cross validation
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
View(results)
#------------------------------#
# Credit Card Fraud Detection
# Thomas Brown
#------------------------------#
library(tidyverse)
library(caret)
set.seed(8549)
# Load into memory the source dataset from kaggles
source <- read.csv("/Users/Thomas/Downloads/creditcard.csv")
# Preview the data
head(source)
# Check the target split
table(source$Class)
sum(table(source$Class[2])) / length(source$Class)
# Perform a train/test split
data_index <- createDataPartition(source$Class, p = 0.8,
list = FALSE, times = 1)
train <- source[data_index,]
test <- source[-data_index,]
rm(source)
# Data Preprocessing: Not Needed due to PCA
#---------------------------------------#
# Stage 1: Exploratory Data Analysis
#---------------------------------------#
# Descriptives
descriptives <- train %>%
group_by(Class) %>%
summarise_all(mean)
# Quick and Dirty Evaluation of the principal components - correlation plot
correlations <- train %>%
summarize_all(funs(cor(., train$Class))) %>%
t()
colnames(correlations) <- "coef"
correlations <- data.frame(correlations)
correlations$feature <- rownames(correlations)
rownames(correlations) <- 1:nrow(correlations)
correlations <- mutate(correlations,
direction = case_when(
coef >= 0 ~ "Positive",
TRUE ~ "Negative"
),
magnitude = abs(coef)) %>%
filter(feature != "Class")
correlations$feature <- factor(correlations$feature) %>%
fct_reorder(correlations$magnitude)
exploratory_plot <- ggplot(data = correlations, aes(x = feature, y = magnitude, fill = direction)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Magnitude of Coefficient",
y = "Feature",
title = "Correlations with outcome",
fill = "Coefficient Direction") +
theme(plot.title = element_text(hjust = 0.5))
exploratory_plot
exploratory_plot + ylim(0, 1)
#---------------------------------------#
# Stage 2: Machine Learning Algorithm
#          construction
#---------------------------------------#
train$Class <- factor(train$Class)
levels(train$Class) <- c("Fraud", "Genuine")
# Model 1: Logistic Regression --------------------------------------------
# Set up a baseline model: Logistic Regression
# Set up 5 fold cross validation
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
# Model 2: Random Forest --------------------------------------------------
# Model 3: Support Vector Machine -----------------------------------------
# Model 4: Gradient Boosting ----------------------------------------------
View(fit)
predictions <- fit$pred
View(predictions)
View(predictions)
View(predictions)
plot(fit)
match <- predictions %>%
match = case_when(
pred == obs ~ 1,
TRUE ~ 0
)
match <- predictions %>%
mutate(match = case_when(
pred == obs ~ 1,
TRUE ~ 0
))
View(predictions)
View(match)
View(predictions)
# Correct Fraudulant Detections
fraud <- filter(match, obs == "Genuine")
sum(fraud$match) / length(fraud$match)
sum(match$match) / length(match$match)
sum(fraud$match)
length(fraud$match)
fp <- filter(match, obs == "Fraud")
sum(fp$match) / length(fp$match)
1 - (sum(fp$match) / length(fp$match))
# Set up 5 fold cross validation
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
predictions <- fit$pred
match <- predictions %>%
mutate(match = case_when(
pred == obs ~ 1,
TRUE ~ 0
))
# Correct Fraudulant Detections
fraud <- filter(match, obs == "Fraud")
sum(match$match) / length(match$match)
sum(fraud$match) / length(fraud$match)
# False Positive
fp <- filter(match, obs == "Genuine")
1 - (sum(fp$match) / length(fp$match))
sum(fraud$match)
train$Class <- factor(train$Class)
levels(train$Class) <- c("Genuine", "Fraud")
# Model 1: Logistic Regression --------------------------------------------
# Set up a baseline model: Logistic Regression
# Set up 5 fold cross validation
tc <- trainControl("cv", 5,
savePredictions = TRUE,
classProbs = TRUE)
fit <- train(factor(Class) ~.,
data      = train    ,
method    = "glm"    ,
family    = binomial ,
trControl = tc,
na.action = na.omit)
results <- round(summary(fit)$coefficients, 3)
predictions <- fit$pred
match <- predictions %>%
mutate(match = case_when(
pred == obs ~ 1,
TRUE ~ 0
))
# Correct Fraudulant Detections
fraud <- filter(match, obs == "Fraud")
sum(match$match) / length(match$match)
sum(fraud$match) / length(fraud$match)
# False Positive
fp <- filter(match, obs == "Genuine")
1 - (sum(fp$match) / length(fp$match))
View(fit)
predict(fit, newdata = train)
actual <- train$Class
predictions <- predict(fit, newdata = train)
temp <- cbind(prediction, actual)
temp <- cbind(predictions, actual)
temp <- cbind(predictions, actual)
temp <- temp %>%
mutate(match = case_when(
predictions == actual ~ 1,
TRUE ~ 0
))
temp <- data.frame(temp) %>%
mutate(match = case_when(
predictions == actual ~ 1,
TRUE ~ 0
))
View(temp)
sum(temp$match) / length(temp$match)
table(temp$match)
table(temp$predictions, temp$actual)
View(fit)
install.packages("tidytext")
library(twitteR)
#--------------------------#
# Twitter User Analysis
# Thomas Brown
#--------------------------#
library(twitteR)
if (Sys.info()[['sysname']] == "Darwin") {
setwd("/Users/Thomas/Dropbox/Data Science/Projects/Twitter")
} else {
setwd("C:/Users/Thomas/Dropbox/Data Science/Projects/Twitter")
}
setup_twitter_oauth(consumer_key = "jXzleremd05nc4y8cXXGXOkQA",
consumer_secret = "IljMpzunOT5sQoadrflkea0hr1vpBEa4KPMyhZrcqaAf6O3Uc6",
access_token = "59942652-Z74Sv5CsJNYEpPAtoYxb3ppmfZlbxgnwIHeAIdj3j",
access_secret = "hPla2hA922mR430txeylaa6AnO7DZQpp4iYAAYwOiBjbI")
user <- "realDonaldTrump"
tweets <- userTimeline(user ,n = 1000, includeRts = FALSE)
View(tweets)
text <- sapply(tweets, function(col) {col$text})
warnings()
text <- apply(tweets, function(col) {col$text})
text <- apply(tweets, FUN = function(col) {col$text})
text <- sapply(tweets, function(col) {col$text})
#--------------------------#
# Twitter User Analysis
# Thomas Brown
#--------------------------#
library(twitteR)
if (Sys.info()[['sysname']] == "Darwin") {
setwd("/Users/Thomas/Dropbox/Data Science/Projects/Twitter")
} else {
setwd("C:/Users/Thomas/Dropbox/Data Science/Projects/Twitter")
}
setup_twitter_oauth(consumer_key = "jXzleremd05nc4y8cXXGXOkQA",
consumer_secret = "IljMpzunOT5sQoadrflkea0hr1vpBEa4KPMyhZrcqaAf6O3Uc6",
access_token = "59942652-Z74Sv5CsJNYEpPAtoYxb3ppmfZlbxgnwIHeAIdj3j",
access_secret = "hPla2hA922mR430txeylaa6AnO7DZQpp4iYAAYwOiBjbI")
user <- "realDonaldTrump"
tweets <- userTimeline(user , n = 1000, includeRts = FALSE)
text <- sapply(tweets, function(col) {col$text})
text
library(tidytext)
text <- data.frame(sapply(tweets, function(col) {col$text}))
View(text)
library(tidyverse)
colnames(text[,1]) <- "tweets"
colnames(text) <- "tweets"
text <- tibble(sapply(tweets, function(col) {col$text}))
colnames(text) <- "tweets"
View(tweets)
text <- as.tibble(sapply(tweets, function(col) {col$text}))
colnames(text) <- "tweets"
text <- tibble(sapply(tweets, function(col) {col$text}))
colnames(text) <- "tweets"
View(text)
text <- unnest_tokens(word, tweets)
tweet_data <- unnest_tokens(text, word, tweets)
View(tweet_data)
tweet_data %>% count
data("stop_words")
tweet_data <- unnest_tokens(text, word, tweets) %>%
anti_join(stop_words)
View(tweet_data)
tweet_data <- unnest_tokens(text, word, tweets) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
View(tweet_data)
tweets <- userTimeline(user , n = 3000, includeRts = FALSE)
View(tweets)
View(tweet_data)
stop_words
devtools::install_github('t-s-brown/ttools')
library(ttools)
View(get_correlations)
library(readr)
X4910797b_ee55_40a7_8668_10efd5c1b960 <- read_csv("Downloads/4910797b-ee55-40a7-8668-10efd5c1b960.csv")
View(X4910797b_ee55_40a7_8668_10efd5c1b960)
source <- read_csv("Downloads/4910797b-ee55-40a7-8668-10efd5c1b960.csv")
View(source)
library(ggplot2)
ggplot(data = source, aes(x = latitiude, y = longitude)) +
geom_point()
ggplot(data = source, aes(x = latitude, y = longitude)) +
geom_point()
ggplot(data = filter(source, latitude != 0), aes(x = latitude, y = longitude)) +
geom_point()
ggplot(data = filter(source, latitude != 0), aes(x = latitude, y = longitude)) +
geom_point()
data <- data %>% filter(latitude != 0)
library(tidyverse)
data <- data %>% filter(latitude != 0)
data <- data %>% filter(latitude != 0)
data <- source %>% filter(latitude != 0)
ggplot(data = filter(source, latitude != 0), aes(x = latitude, y = longitude)) +
geom_point()
data <- source %>% filter(longitude >10)
ggplot(data = data, aes(x = latitude, y = longitude)) +
geom_point()
ggplot(data = data, aes(x = latitude, y = longitude)) +
geom_point(alpha = 0.6)
library(readr)
X0bf8bc6e_30d0_4c50_956a_603fc693d966 <- read_csv("Downloads/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv")
View(X0bf8bc6e_30d0_4c50_956a_603fc693d966)
target <- read_csv("Downloads/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv")
nd <- left_join(source, target, by = "id")
data <- nd %>% filter(longitude >10)
ggplot(data = data, aes(x = latitude, y = longitude)) +
geom_point(alpha = 0.6)
ggplot(data = data, aes(x = latitude, y = longitude, col = status_group)) +
geom_point(alpha = 0.6)
ggplot(data = data, aes(x = latitude, y = longitude, col = status_group)) +
geom_point(alpha = 0.2)
data <- nd %>% filter(longitude >10) %>% sample_frac(size = 0.4)
ggplot(data = data, aes(x = latitude, y = longitude, col = status_group)) +
geom_point(alpha = 0.2)
data <- nd %>% filter(longitude >10) %>% sample_frac(size = 0.05)
ggplot(data = data, aes(x = latitude, y = longitude, col = status_group)) +
geom_point(alpha = 0.2)
View(nd)
install.packages(c("bindr", "bindrcpp", "bookdown", "broom", "caret", "cluster", "config", "curl", "DBI", "dbplyr", "devtools", "forcats", "hms", "httpuv", "keras", "knitr", "lava", "lubridate", "Matrix", "openssl", "pillar", "plogr", "profvis", "psych", "randomForest", "Rcpp", "reticulate", "rfUtilities", "rlang", "rmarkdown", "selectr", "servr", "sfsmisc", "stringi", "stringr", "tfruns", "tidyselect", "tidytext", "timeDate", "tokenizers", "withr", "yaml"))
devtools::install_github('t-s-brown/ttools')
library(tidyverse)
install.packages(c("foreign", "survival"))
install.packages(c('tidyverse', 'caret'))
av_key <- '8GOVCO6D7NT5N3FR'
library(tidyverse)
# API Keys
av_key <- '8GOVCO6D7NT5N3FR'
av_symbol <- 'MSFT'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv')
data <- read_csv(av_url)
View(data)
av_symbol <- 'AAPL'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv')
data <- read_csv(av_url)
View(data)
av_symbol <- 'AAPL'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
av_symbol <- 'BARC.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
av_symbol <- 'BP..L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
av_symbol <- 'BP.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
setwd("~/Dropbox/Data Science/Projects")
ftse100 <- read_csv('FTSE100CODES.csv')
View(ftse100)
ftse100 <- read_csv('FTSE100CODES.csv')[,1]
ftse100 <- read_csv('FTSE100CODES.csv') %>% select(Code)
View(ftse100)
View(ftse100)
ftse100 <- read_csv('FTSE100CODES.csv') %>% select(Code) %>% filter(!is.na(Code)) %>%
View(ftse100)
ftse100 <- read_csv('FTSE100CODES.csv') %>% select(Code) %>% filter(!is.na(Code))
View(ftse100)
av_symbol <- 'BT.A.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
av_symbol <- 'BT.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
av_symbol <- 'BTA.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
av_symbol <- 'BT-A.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(ftse100)
av_symbol <- 'AV-.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
av_symbol <- 'AV.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
av_symbol <- 'HL.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
View(ftse100)
str_detect(ftse100, '.$')
str_detect(ftse100, '\.$')
str_detect(ftse100, '\\.$')
str_detect(ftse100$Code, '.$')
str_detect(ftse100$Code, '\.$')
str_detect(ftse100$Code, '\\.$')
ftse100$Code[str_detect(ftse100$Code, '\\.$')] <- '0'
#----------------------------------#
# Equity Analysis
# Loading Key Technical Data
#----------------------------------#
# Load the required packages
library(tidyverse)
# Set up the working directory
setwd("~/Dropbox/Data Science/Projects")
# API Keys
av_key <- '8GOVCO6D7NT5N3FR'
# Get the FTSE100 List
ftse100 <- read_csv('FTSE100CODES.csv') %>% select(Code) %>% filter(!is.na(Code))
#ftse100$Code[str_detect(ftse100$Code, '\\.$')]
av_symbol <- 'HL.L'
av_url <- paste0('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=',
av_symbol,'&apikey=',av_key,'&datatype=csv&outputsize=full')
data <- read_csv(av_url)
View(data)
